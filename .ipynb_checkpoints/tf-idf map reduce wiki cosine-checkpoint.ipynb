{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e263a9ba-31a8-4d21-a686-521f7af664a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process took 1.5914746999987983 seconds\n",
      "Top keywords for each article: {'Article_1': ['lion', 'mw', 'parser', 'output', 'toc', 'vector', 'panthera', 'africa', 'doi', 'pride', 'leo', 'nation', 'cat', 'list', 'li', 'male', 'origin', 'archiv', 'id', 'african'], 'Article_2': ['tiger', 'mw', 'parser', 'output', 'doi', 'tigri', 'panthera', 'toc', 'vector', 'cat', 'conserv', 'popul', 'li', 'rang', 'speci', 'list', 'hlist', 'thapar', 'pp', 'mi'], 'Article_3': ['beetl', 'mw', 'parser', 'insect', 'output', 'toc', 'vector', 'coleoptera', 'speci', 'doi', 'li', 'id', 'larva', 'list', 'class', 'level', 'edit', 'use', 'item', 'hlist']}\n",
      "Similarity matrix:\n",
      " [[1.         0.46680648 0.25694226]\n",
      " [0.46680648 1.         0.2507467 ]\n",
      " [0.25694226 0.2507467  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import re\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "MAX_WORKERS = 16\n",
    "\n",
    "def get_wikipedia_article(wiki_article, timeout=10):\n",
    "    '''\n",
    "    Method that gets a Wikipedia page.\n",
    "    '''\n",
    "    wiki_page_url = 'https://en.wikipedia.org/wiki/' + wiki_article\n",
    "    response = requests.get(url=wiki_page_url, timeout=timeout)\n",
    "    article = response.text\n",
    "    return article\n",
    "\n",
    "def get_wikipedia_articles(wiki_articles):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(get_wikipedia_article, wiki_article=article) for article in wiki_articles]\n",
    "        articles = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    return articles\n",
    "\n",
    "# Functions to clean HTML, punctuation marks and symbols from the original articles\n",
    "def clean_html(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def clean_punctuation_marks(text):\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "def clean_return_and_tabs(text):\n",
    "    return re.sub(r'[\\n\\t]', ' ', text)\n",
    "\n",
    "def clean_article(data):\n",
    "    cdata = clean_html(data)\n",
    "    cdata = clean_punctuation_marks(cdata)\n",
    "    cdata = clean_return_and_tabs(cdata)\n",
    "    return cdata\n",
    "\n",
    "def remove_stopwords(document, stopwords):\n",
    "    return [word for word in document if word not in stopwords]\n",
    "\n",
    "def remove_numbers(document):\n",
    "    return [word for word in document if not any(char.isdigit() for char in word)]\n",
    "\n",
    "def remove_wiki_and_wg_words(document):\n",
    "    return [word for word in document if not word.startswith('wg') and not word.startswith('wiki')]\n",
    "\n",
    "def process_article(article, stopwords, stemmer):\n",
    "    article = article.lower()\n",
    "    article = clean_article(article)\n",
    "    tokens = tokenize.word_tokenize(article)\n",
    "    tokens = remove_stopwords(tokens, stopwords)\n",
    "    tokens = list(map(stemmer.stem, tokens))\n",
    "    tokens = remove_numbers(tokens)\n",
    "    tokens = remove_wiki_and_wg_words(tokens)\n",
    "    return tokens\n",
    "\n",
    "def map_phase(wiki_articles, stopwords, stemmer):\n",
    "    articles = get_wikipedia_articles(wiki_articles)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        processed_articles = list(executor.map(lambda article: process_article(article, stopwords, stemmer), articles))\n",
    "    return processed_articles\n",
    "\n",
    "def reduce_phase(processed_articles):\n",
    "    # Combine all processed words into a single list\n",
    "    all_words = [word for article in processed_articles for word in article]\n",
    "\n",
    "    # Convert the list of words into space-separated strings for TfidfVectorizer\n",
    "    corpus = [' '.join(article) for article in processed_articles]\n",
    "\n",
    "    # Initialize TfidfVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Compute TF-IDF scores\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Get feature names (words)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a dictionary to store top 20 keywords for each article\n",
    "    top_keywords = {}\n",
    "\n",
    "    # Iterate over each article\n",
    "    for i, article in enumerate(processed_articles):\n",
    "        # Get the TF-IDF scores for the current article\n",
    "        article_tfidf_scores = tfidf_matrix[i].toarray()[0]\n",
    "\n",
    "        # Create a dictionary to store TF-IDF scores for each word\n",
    "        word_tfidf_scores = {word: score for word, score in zip(feature_names, article_tfidf_scores)}\n",
    "\n",
    "        # Sort words by their TF-IDF scores in descending order\n",
    "        sorted_words = sorted(word_tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Extract top 20 keywords\n",
    "        top_keywords[f'Article_{i+1}'] = [word for word, score in sorted_words[:20]]\n",
    "\n",
    "    return top_keywords, tfidf_matrix\n",
    "\n",
    "def calculate_similarity(tfidf_matrix):\n",
    "    # Calculate cosine similarity between all articles\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "# Main workflow\n",
    "if __name__ == \"__main__\":\n",
    "    wiki_articles = ['lion', 'beetle', 'tiger']\n",
    "\n",
    "    stopwords = set(['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during',\n",
    "                     'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours',\n",
    "                     'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as',\n",
    "                     'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your',\n",
    "                     'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should',\n",
    "                     'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when',\n",
    "                     'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does',\n",
    "                     'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not',\n",
    "                     'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which',\n",
    "                     'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by',\n",
    "                     'doing', 'it', 'how', 'further', 'was', 'here', 'than', '_s', '_j', '_m', '_a'])\n",
    "\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # Map phase\n",
    "    processed_articles = map_phase(wiki_articles, stopwords, stemmer)\n",
    "\n",
    "    # Reduce phase\n",
    "    top_keywords, tfidf_matrix = reduce_phase(processed_articles)\n",
    "\n",
    "    # Calculate similarity\n",
    "    similarity_matrix = calculate_similarity(tfidf_matrix)\n",
    "\n",
    "    stop = time.perf_counter()\n",
    "    print('The process took', stop - start, 'seconds')\n",
    "    print('Top keywords for each article:', top_keywords)\n",
    "    print('Similarity matrix:\\n', similarity_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
